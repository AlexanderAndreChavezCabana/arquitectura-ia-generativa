{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Sesión 1: Fundamentos y Arquitecturas de IA Generativa\n",
    "\n",
    "Este notebook contiene ejercicios prácticos para:\n",
    "- Exploración de modelos en Hugging Face Hub\n",
    "- Instalación y carga de modelos pre-entrenados\n",
    "- Generación controlada con Transformers\n",
    "- Experimentación con parámetros de generación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "Primero instalamos las bibliotecas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar transformers y otras dependencias\n",
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración de Hugging Face Hub\n",
    "\n",
    "Importamos las bibliotecas y exploramos modelos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import list_models\n",
    "import torch\n",
    "\n",
    "# Verificar disponibilidad de GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de Modelo Pre-entrenado\n",
    "\n",
    "Cargamos un modelo ligero para experimentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo (ejemplo con GPT-2 o Phi-3-mini)\n",
    "model_name = \"gpt2\"  # Cambiar por \"microsoft/Phi-3-mini-4k-instruct\" o \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"Modelo {model_name} cargado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generación Básica de Texto\n",
    "\n",
    "Realizamos una generación simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt inicial\n",
    "prompt = \"La inteligencia artificial generativa es\"\n",
    "\n",
    "# Tokenizar\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Decodificar y mostrar\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentación con Parámetros\n",
    "\n",
    "Probamos diferentes configuraciones de generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentar con temperature, top_p, top_k\n",
    "prompt = \"En el futuro, la IA generativa permitirá\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Parámetros a experimentar\n",
    "configs = [\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 50},\n",
    "    {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0},\n",
    "    {\"temperature\": 0.3, \"top_p\": 0.5, \"top_k\": 20}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\n--- Configuración {i+1} ---\")\n",
    "    print(f\"Parámetros: {config}\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        **config,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Resultado: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering: Zero-shot y Few-shot\n",
    "\n",
    "Experimentamos con diferentes técnicas de prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot\n",
    "zero_shot_prompt = \"Clasifica el siguiente texto como positivo o negativo: 'Este producto es excelente'\"\n",
    "\n",
    "# Few-shot\n",
    "few_shot_prompt = \"\"\"Clasifica los siguientes textos:\n",
    "Texto: 'Me encanta este producto' - Sentimiento: Positivo\n",
    "Texto: 'Muy decepcionado con la compra' - Sentimiento: Negativo\n",
    "Texto: 'Excelente calidad y precio' - Sentimiento: Positivo\n",
    "Texto: 'No lo recomiendo para nada' - Sentimiento:\"\"\"\n",
    "\n",
    "# Probar ambos\n",
    "for prompt_type, prompt in [(\"Zero-shot\", zero_shot_prompt), (\"Few-shot\", few_shot_prompt)]:\n",
    "    print(f\"\\n--- {prompt_type} ---\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=30, temperature=0.3)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejercicio Práctico\n",
    "\n",
    "**Desafío**: Implementa un chatbot simple que responda preguntas sobre un tema específico usando las técnicas aprendidas.\n",
    "\n",
    "- Carga un modelo más avanzado (Llama-3.1-8B, Qwen2.5-7B o Phi-3-mini)\n",
    "- Diseña prompts efectivos para un caso de uso específico\n",
    "- Experimenta con los parámetros para optimizar las respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
